# Latency And Throughput

**Latency** refers to how long does it take for data to traverse a system (from one point to another in the system). Ex: Time for request to go from **Client** to **Server** and back

When you design a system, ideally you want to minimize *latency*, but it depends on how other components in the system have been designed. What trade-offs are being made.
Ex: lag in a video game is an example of **high latency**

**Throughput** is the amount of work that a machine can handle in a given period of time.
Ex: How much data can be transferred from one point in a system to another in a given amount of time. If we make request from client to a server - how much requests can the server handle in a given amount of time?

> Latency, bandwidth, and throughput are all interrelated, but they all measure different things.

![Key Terminologies](ImageRepo/Latency_Through.png?raw=true)
